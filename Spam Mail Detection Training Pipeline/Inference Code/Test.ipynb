{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100b15ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Loading models and vectorizers...\n",
      "âœ… Models loaded successfully!\n",
      "\n",
      "ðŸ“„ Reading data from: spam_emails_robust.csv\n",
      "\n",
      "ðŸ“Š HAM: 0\n",
      "ðŸ“¬ SPAM: 4255\n",
      "âœ… Saved predictions to: csv_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dev\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Dev\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\lightgbm\\basic.py:1238: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "===================================================\n",
    " Email Spam Detection Prediction Pipeline\n",
    "---------------------------------------------------\n",
    " Author : Devashish\n",
    " Purpose: Deployment-ready inference pipeline\n",
    " Notes  : Optimized for API / Microservice integration\n",
    "===================================================\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "# ===============================\n",
    "# Initialization (Run Once)\n",
    "# ===============================\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Paths (relative or configurable via ENV)\n",
    "MODEL_DIR = \"models\"\n",
    "\n",
    "# Load all models once at startup\n",
    "print(\"ðŸš€ Loading models and vectorizers...\")\n",
    "nb_model = joblib.load(os.path.join(MODEL_DIR, \"nb_model.pkl\"))\n",
    "lr_model = joblib.load(os.path.join(MODEL_DIR, \"lr_model.pkl\"))\n",
    "lgb_model = joblib.load(os.path.join(MODEL_DIR, \"lgb_model.pkl\"))\n",
    "tfidf = joblib.load(os.path.join(MODEL_DIR, \"tfidf_vectorizer.pkl\"))\n",
    "le = joblib.load(os.path.join(MODEL_DIR, \"label_encoder.pkl\"))\n",
    "\n",
    "print(\"âœ… Models loaded successfully!\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Utility Functions\n",
    "# ===============================\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Cleans raw email text for TF-IDF transformation.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '<URL>', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '<EMAIL>', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s<>]', '', text)\n",
    "    text = ' '.join([w for w in text.split() if w not in stop_words])\n",
    "    return text\n",
    "\n",
    "\n",
    "def compute_features(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Compute additional numeric and keyword-based features.\"\"\"\n",
    "    df['num_links'] = df['content'].str.count(r'http\\S+|www\\S+')\n",
    "    df['num_exclamations'] = df['content'].str.count('!')\n",
    "    df['num_uppercase_words'] = df['content'].apply(lambda x: sum(1 for w in x.split() if w.isupper()))\n",
    "    df['text_length'] = df['content'].str.len()\n",
    "    df['num_special_chars'] = df['content'].str.count(r'[^a-zA-Z0-9\\s]')\n",
    "\n",
    "    spam_words = ['free', 'win', 'click', 'prize', 'buy now']\n",
    "    for word in spam_words:\n",
    "        df[f'has_{word.replace(\" \", \"_\")}'] = df['content'].str.contains(word, case=False).astype(int)\n",
    "\n",
    "    return df[['num_links', 'num_exclamations', 'num_uppercase_words',\n",
    "               'text_length', 'num_special_chars'] +\n",
    "              [f'has_{w.replace(\" \", \"_\")}' for w in spam_words]].values\n",
    "\n",
    "\n",
    "def ensemble_predict(X_combined):\n",
    "    \"\"\"Run ensemble prediction (NB + LR + LGBM).\"\"\"\n",
    "    nb_probs = nb_model.predict_proba(X_combined)\n",
    "    lr_probs = lr_model.predict_proba(X_combined)\n",
    "    lgb_probs = lgb_model.predict_proba(X_combined)\n",
    "\n",
    "    ensemble_probs = (0.3 * nb_probs + 0.4 * lr_probs + 0.3 * lgb_probs)\n",
    "    pred_label = ensemble_probs.argmax(axis=1)\n",
    "    confidence = ensemble_probs.max(axis=1) * 100\n",
    "\n",
    "    labels = le.inverse_transform(pred_label)\n",
    "    return labels, confidence\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Core Prediction Methods\n",
    "# ===============================\n",
    "def predict_single(subject: str, body: str) -> Dict[str, Union[str, float]]:\n",
    "    content = f\"{subject} {body}\".strip()\n",
    "    clean_content = clean_text(content)\n",
    "\n",
    "    X_text = tfidf.transform([clean_content])\n",
    "\n",
    "    # Handcrafted numeric features\n",
    "    num_links = len(re.findall(r'http\\S+|www\\S+', content))\n",
    "    num_exclamations = content.count('!')\n",
    "    num_uppercase_words = sum(1 for w in content.split() if w.isupper())\n",
    "    text_length = len(content)\n",
    "    num_special_chars = len(re.findall(r'[^a-zA-Z0-9\\s]', content))\n",
    "    spam_words_vals = [int(word in content.lower()) for word in ['free', 'win', 'click', 'prize', 'buy now']]\n",
    "\n",
    "    X_hand = np.array([[num_links, num_exclamations, num_uppercase_words,\n",
    "                        text_length, num_special_chars] + spam_words_vals])\n",
    "    X_combined = hstack([X_text, X_hand])\n",
    "\n",
    "    label, confidence = ensemble_predict(X_combined)\n",
    "    return {\"label\": label[0], \"confidence\": round(confidence[0], 2)}\n",
    "\n",
    "\n",
    "def predict_batch(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Predict multiple emails from a DataFrame (CSV or API batch input).\n",
    "    Expected columns: 'subject', 'body'\n",
    "    \"\"\"\n",
    "    required_cols = {'subject', 'body'}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(f\"CSV must contain columns: {required_cols}\")\n",
    "\n",
    "    df['content'] = df['subject'].fillna('') + ' ' + df['body'].fillna('')\n",
    "    df['clean_content'] = df['content'].apply(clean_text)\n",
    "\n",
    "    X_text = tfidf.transform(df['clean_content'])\n",
    "    X_hand = compute_features(df)\n",
    "    X_combined = hstack([X_text, X_hand])\n",
    "\n",
    "    labels, confidence = ensemble_predict(X_combined)\n",
    "\n",
    "    df['predicted_label'] = labels\n",
    "    df['confidence'] = np.round(confidence, 2)\n",
    "    return df[['subject', 'predicted_label', 'confidence']]\n",
    "\n",
    "\n",
    "def predict_from_csv(csv_path: str, output_path: str = \"csv_predictions.csv\"):\n",
    "    \"\"\"\n",
    "    Read emails from CSV, predict spam/ham, save results.\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ“„ Reading data from: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    results_df = predict_batch(df)\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "\n",
    "    spam_count = (results_df[\"predicted_label\"].str.lower() == \"spam\").sum()\n",
    "    ham_count = (results_df[\"predicted_label\"].str.lower() == \"ham\").sum()\n",
    "\n",
    "    print(f\"\\nðŸ“Š HAM: {ham_count}\")\n",
    "    print(f\"ðŸ“¬ SPAM: {spam_count}\")\n",
    "    print(f\"âœ… Saved predictions to: {output_path}\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Main Entry (Local Testing)\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = \"spam_emails_robust.csv\"\n",
    "    predict_from_csv(csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
